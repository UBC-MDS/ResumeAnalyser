{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Introduction to resume analyser (Prabhjit)\n",
    "\n",
    "Explain at a high level what this package aims to do and explain the example\n",
    "\n",
    "Example: Comparing resume I downloaded from www.heinz.cmu.edu to the Software Engineer Job Description Sampson sent on Slack (**citation needed**)\n",
    "If we need a better resume to cite we can do it too if someone can find a better one! Or does one of us want to upload an anonymised version of our own resumes? Also, should we add a reference section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Resume Analyser\n",
    "\n",
    "We will be assuming that you have already installed our resumeanalyser package, as per the `Installation` section of our README.md file. To start using resume analyser in a project, we will run the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n"
     ]
    }
   ],
   "source": [
    "import resumeanalyser\n",
    "print(resumeanalyser.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text \n",
    "To start comparing the sample resume to the sample job description, we will first start by extracting the text from them. The sample resume, which was downloaded from Carnegie Mellon University's Heinz College, has been stored as a PDF in the `data` subdirectory under the `tests` directory of this repository, while the sample job description has been stored as a docx document under the same directory. Thankfully, our `resumeanalyser` is compatible with both .docx and .pdf documents. We will thus start the analysis by reading in the text from both sources, which will serve as the basis of our analysis for the rest of this vignette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import the text extraction functions\n",
    "from resumeanalyser.text_reading import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our imports are done, we will be using `pdf_to_text` to read the text from the sample resume, and will store this as a string. This function takes in a pathname ending in `.pdf` as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polly Seapsea@andrew.cmu.edu | 412.889.4687 | link\n"
     ]
    }
   ],
   "source": [
    "resume_pdf_path = \"../tests/data/msppm-sample-resume.pdf\"\n",
    "sample_resume_text = pdf_to_text(resume_pdf_path)\n",
    "print(sample_resume_text[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resume has now been read and stored under the variable `sample_resume_text`. Next, we will be using the function `docx_to_text` to extract text from the job description, and will store the text as a string. Similarly, it takes in a path name ending in `.docx` as an input,so if you run into problems with either of the text reading functions, please be sure to check your file path name and use the appropriate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineer Job Description We are looking f\n"
     ]
    }
   ],
   "source": [
    "job_desc_path = \"../tests/data/software_engineer_job_description.docx\"\n",
    "job_desc_text = docx_to_text(job_desc_path)\n",
    "print(job_desc_text[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage of Text Cleaning Functions\n",
    "\n",
    "`resumeanalyser` offers how to use a series of text cleaning functions. These functions include:\n",
    "1. Removing punctuation\n",
    "2. Tokenization\n",
    "3. Converting to lower case\n",
    "4. Removing stop words\n",
    "5. Lemmatization\n",
    "\n",
    "You can apply these functions either step-by-step to understand each part of the text cleaning process, \n",
    "or you can use the `clean_text` function to apply all these steps in one go for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK WordNet and stopwords downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from resumeanalyser.text_cleaning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "sample_text = \"The cats are chasing the mice, and one mouse is running faster than the others.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating step-by-step process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Punctuation: The cats are chasing the mice and one mouse is running faster than the others\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Remove punctuation\n",
    "no_punctuation = remove_punctuation(sample_text)\n",
    "print(\"Text without Punctuation:\", no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: ['The', 'cats', 'are', 'chasing', 'the', 'mice', 'and', 'one', 'mouse', 'is', 'running', 'faster', 'than', 'the', 'others']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize the text\n",
    "tokens = tokenize(no_punctuation)\n",
    "print(\"Tokenized Text:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase Tokens: ['the', 'cats', 'are', 'chasing', 'the', 'mice', 'and', 'one', 'mouse', 'is', 'running', 'faster', 'than', 'the', 'others']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Convert to lower case\n",
    "lower_tokens = to_lower(tokens)\n",
    "print(\"Lowercase Tokens:\", lower_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without Stop Words: ['cats', 'chasing', 'mice', 'one', 'mouse', 'running', 'faster', 'others']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Remove stop words\n",
    "no_stop_words = remove_stop_words(lower_tokens)\n",
    "print(\"Tokens without Stop Words:\", no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['cat', 'chasing', 'mouse', 'one', 'mouse', 'running', 'faster', 'others']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Lemmatize\n",
    "lemmatized_tokens = lemmatize(no_stop_words)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the clean_text function for an all-in-one solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: cat chasing mouse one mouse running faster others\n"
     ]
    }
   ],
   "source": [
    "from resumeanalyser.text_cleaning import clean_text\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage of Metric Functions for Comparing two texts\n",
    "\n",
    "`resumeanalyser` offers two functions to compare the two texts provided by the user. These functions include:\n",
    "1. Syntactic Text Matching\n",
    "2. Semantic Text Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literal Text Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literal Text Matching\" typically refers to a measure of how closely two pieces of text align in a character-by-character or word-by-word manner without considering variations or synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresumeanalyser\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityCV\n\u001b[1;32m      3\u001b[0m literal_match_score \u001b[38;5;241m=\u001b[39m SimilarityCV(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am studying Data Science at UBC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are many good sources to study Data Science online\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiteral Match Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, literal_match_score)\n",
      "File \u001b[0;32m~/git/mds/524_DSCI/resumeanalyser/src/resumeanalyser/metrics.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from resumeanalyser.metrics import SimilarityCV\n",
    "\n",
    "literal_match_score = SimilarityCV(\"I am studying Data Science at UBC\", \"There are many good sources to study Data Science online\")\n",
    "print(\"Literal Match Score:\", literal_match_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Text Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Text Matching measures the similarity in meaning between two pieces of text. Unlike literal or exact match scores, semantic matching takes into account the context, synonyms, and related concepts to determine how closely the content aligns in terms of intent or significance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resumeanalyser.metrics import SimilaritySpacy\n",
    "\n",
    "semantic_match_score = SimilaritySpacy(\"I am studying Data Science at UBC\", \"There are many good sources to study Data Science online\")\n",
    "print(\"Syntactic Match Score:\", semantic_match_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Using Plotting Functions of the Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resumeanalyser.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'I am going to fill in a test text here the the the a a a a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can plot the word cloud of the input resume/job description text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plot_wordcloud(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or plot the top-frenquency words that are most relvant in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plot_topwords(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot both in one suite plot for illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plot_suite(test_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
